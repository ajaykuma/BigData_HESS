Setting Apache Hadoop core (open-source distribution) on multiple linux machines
----------------------
#Setup multiple machines:

refer : (if needed)
Setup Linux machines within VMBOX-ubuntu.txt or 
Setup Linux machines within VMBOX-centos.txt

(virtual machines within vmbox/vmware)
(AWS ec2 machines)
(GCP instances)
(physical machines)
--machines should be able to ping each other
--machines should be able to connect to internet (can be optional if all packages downloaded)
--machines should have ssh access to each other and to themselves
--firewall should be turned off/edited with rules to allow ports
--better to give ssh access for root user (if setting up things for root user)
--download oracleJDK/openJDK 1.8 or greater and setup path of java in .bashrc for root n other user
--additionally install packages such as NTP(network time protocol),wget,git,openssh-server,vim

#commands to perform basic setup(for each machine)
#commands are as per Ubuntu (if doing it on centos/rhel use yum related commands)

#on machines
#login as root
$sudo su
$apt-get install vim
$apt-get install wget
$apt-get install ntp
$apt-get install openssh-server
$apt-get update

#ubuntu ---disable firewall (old version 16.x)
$iptables --flush
$iptables stop
or
$ufw disable (new version 18.x onwards)

$ifconfig ---get your ipaddress(old version 16.x)
$ip address (new version 18.x onwards)
$hostname ---get your hostname

#update your /etc/hosts
127.0.0.1     localhost
ipaddress     hostname

#sudo visudo and add 'hdu' as user next to root where you see ALL
#This will give your user sudo access

#Download oracle jdk from oracle site
#create a directory under /usr/lib
$cd /usr/lib
$mkdir jvm
$cd jvm
$sudo tar -xvf /home/hdu/Downloads/jdk****
$sudo chown -R root:root jdk***

#update your java path in your user's .bashrc
(ex: my user hdu)
$su - hdu

$vi .bashrc
export JAVA_HOME=/usr/lib/jvm/jdk****
export PATH=$PATH:$JAVA_HOME/bin

#save your .bashrc
esc+shift+:+wq

#refresh it by  
$source .bashrc

$java -version ( does it show your java version)

#create a dedicated user for ur hadoop setup
#ensure your logged in as root
#adduser hdu
--
---
--

#Now Generate ssh keys for your user on ubuntu

#Commands:
--------------
#To have SSH access across machines
#1.generate ssh keys using 
$ssh-keygen -t rsa -P ""

#2.Distributing ssh public keys to other machines
----------------------
hdu$machine1:ssh-copy-id -i $HOME/.ssh/id_rsa.pub hdu@machine2

#3.To enable SSH access to your local machine with this newly created key
----------------------------------------
#to add the xxxx@masters public SSH key (which should be in $HOME/.ssh/id_rsa.pub) 
#to the authorized_keys file of xxxx@slave(in this users $HOME/.ssh/authorized_keys)
hdu$machine1:cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

#download hadoop tar from http://archive.apache.org/dist/hadoop/common/
#Lets get hadoop-2.7.2(or new stable version)
#To untar and create dir ,to create link and to change owner of directory
-----------------------------
#Lets untar it in /usr/local
$cd /usr/local
$sudo tar -xvf hadoop-2.x.x.tar.gz
$sudo ln -s hadoop-2.x.x hadoop
$sudo chown -R hdu:hdu hadoop* -----------------(am making user -hdu as owner and will be admin for hadoop)
$cd

#update hadoop path in .bashrc for 'hdu' user
vi .bashrc
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin

save your .bashrc
refresh it ----> source .bashrc

hadoop version ( does it show your hadoop version)
=========================================================

#Editing config files to setup cluster

#Note** sample configs for hadoop-2.6.5 and hadoop-2.7.2 are already provided which can directly be copied into your hadoop directories on nodes.

config files:(you can use ports 8020 for core and 8021 for mapred)
or
config files:(you can use ports 9000 for core and 9001 for mapred)

$cd /usr/local/hadoop/etc/hadoop

**replace hostname with name of your node where you want to run a particular process/daemon

core-site.xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://hostname:9000</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.Here hostname points to node where your NN will run</description>
</property>

mapred-site.xml
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
  </property>

--change replication as per number of nodes you want to make datanodes
hdfs-site.xml
<property>
  <name>dfs.replication</name>
  <value>x</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

**to have namenode or datanode data in specific directories,specify path in hdfs-site.xml,
  if not everything will get stored in /tmp
**in Apache we need to create the /abc directory,rest will be created by hadoop.
** we need to make sure the ownership of /abc dir is same as your hadoop directory
**for example as below
<property>
<name>dfs.namenode.name.dir</name>
<value>/abc/namenode</value>
<final>true</final>
</property>


<property>
<name>dfs.datanode.data.dir</name>
<value>/abc/datanode</value>
<final>true</final>
</property>

**specify settings of http ports used by namenode and secondarynamenode in hdfs-site.xml
<property>
<name>dfs.namenode.http-address</name>
<value>nnhostname:50070</value>
</property>

<property>
<name>dfs.namenode.secondary.http-address</name>
<value>snnhostname:50090</value>
</property>

**specific properties for secondarynamenode,to be given in hdfs-site.xml of node where your SNN 
  will run
**Http properties are important,rest if not given will make hadoop consider defaults
<property>
<name>dfs.namenode.http-address</name>
<value>nnhostname:50070</value>
</property>

<property>
<name>dfs.namenode.secondary.http-address</name>
<value>snnhostname:50090</value>
</property>

<property>
<name>dfs.namenode.checkpoint.dir</name>
<value>/abc/snnfsi</value>
</property>

<property>
<name>dfs.namenode.checkpoint.edits.dir</name>
<value>/abc/snnedits</value>
</property>

<property>
<name>dfs.namenode.checkpoint.period</name>
<value>600</value>
</property>

yarn-site.xml
<property>
<name>yarn.resourcemanager.address</name>
<value>rmhostname:9001</value>
</property>

<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>rmhostname:8031</value>
</property>

<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>

slaves
dnhostname1
dnhostname2
dnhostname3

Summary : Note**
If singlenode cluster
-then update all these properties with same hostname
-create relevant directories
-check if you can ssh to hostname
-check if firewall disabled
-check permissions of hadoop and relevant directories
 then proceeed..

If multinode cluster
-create relevant directories
-check if you can ssh to hostname and other machines hostname
-check if firewall disabled
-check permissions of hadoop and relevant directories

core-site.xml (remains same in all machines)
hdfs-site.xml (contains properties as per daemons in machines)
yarn-site.xml (remains same in all machines)
mapred-site.xml (remains same in all machines)
slaves (remains same in all machines)

if hadoop path was setup in .bashrc

also create directories on each machine
$sudo mkdir /orgz
$sudo chown -R hdu:hdu /orgz

--on namenode machine
$hdfs namenode -format

check if /abc now has namenode directory created

--on datanodes or machine for snn
/abc will have datanode or snn related directories created when you start the cluster

you can start your cluster by
if namenode and resourcemanager configured to run on same machine then..
$start-dfs.sh
$start-yarn.sh

if namenode different than resourcemanager
--on namenode
$start-dfs.sh

--on resourcemanager
$start-yarn.sh

or
if sbin path was not set
$cd /usr/local/hadoop
$sbin/start-dfs.sh

**(if rm is on different node than nn, remember to updates slaves file and start rm there
either by sbin/start-yarn.sh & if slaves file not updated then by  
sbin/yarn-daemon.sh start resourcemanager )

$sbin/start-yarn.sh

#to start history server
$ $HADOOP_HOME/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONFIG_DIR start historyserver

============================================================
Additionals (optional)

#to setup hadoop 2 you can also use this way(generally above mentioned way is preferred)

mapred-site.xml.template
<property>
<name>mapred.job.tracker</name>
<value>aj4:9001</value>
</property>

<property>
<name>mapred.framework.name</name>
<value>yarn</value>
</property>

and in yarn-site.xml
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>rmhostname:8031</value>
</property>

<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>

Note**https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/PluggableShuffleAndPluggableSort.html

============================================================
#For rack awareness:
Update parameter in hdfs-site.xml (if not done already and restart hdfs related processes)

<property>
<name>topology.script.file.name</name>
<value>/location/topology.sh</value>
</property>

----------------------------------------------
#create a file in home directory
 /usr/local/hadoop/topology.data

ip address 1(dn1)     /rack1
ip address 2(dn2)     /rack2

and similarly....

or i can give more information like
ip address 1(dn1)     /sw1/rack1
ip address 2(dn2)     /sw2/rack2
---------------------------------------------
#create a topology script in conf directory

/usr/local/hadoop/etc/hadoop/topology.sh :

#!/bin/bash
while [ $# -gt 0 ] ; do
   nodeArg=$1
   exec< path/topology.data
   result=""
   while read line ; do
       ar=( $line )
       if [  "${ar[0]}" = "$nodeArg" ] ; then
       result="${ar[1]}"
       fi
   done
   shift
   if [ -z "$result" ]; then
     echo -n "/default"
    else
     echo -n "$result"
   fi
   done

-------------------------------------------
#Steps to perform upgrade (check before proceeding, as this might have changed as per your version)
Hadoop upgrade:

To save status of hadoop 1 cluster
1.hadoop fsck /
2.hadoop fsck / - files - blocks -locations
3.backing up for comparision later
  hadoop fsck / - files - blocks - locations > fsck.bck
4.backing up list of files recursively
  hadoop fs -lsr / > filename.bck
5.backing up datanodes information to compare later if everything is consistent
  hadoop dfsadmin -report > layout.bck
6.Checking if there is any pending upgrade from last change
  hadoop dfsadmin -upgradeProgress status
  NO uncmiited change from last upgrade,make sure it shows "there is no upgrade in progress"

Also check in metadata and data path for existnce of 'previous' directory

if so,then first finalize upgrade and then proceed..

7.stop daemons.

8.Download new version hadoop package

9.untar it.

10.copy all config files from old version to new version directory

   cp /usr/local/hadoop/conf/xx  /usr/local/hadoopnew/conf/

11.check if symlink exists,if not create one using
   ln -s hadoop hadaj

   if exits, unlink it using
   unlink hadoop
   then create new symlink pointing to new directory
   ln -s hadoopnew hadaj

this avoids editing .bash_profile/.bash_rc etc

12.note** no fomatting namenode

13.start daemon namenode

hadoop-daemon.sh start namenode -upgrade

14. CHECK UPGRADE STATUS

15.START DAEMON DATANODE

16.Finally check for dfsadmin report and safemode status.

17. and the cluster can be left in same state or the upgrade has to be finalized.( which removes 
all previous enteries)

18. if there is any case of roll back,
hadoop-daemon.sh start namenode -rollback

19. if no roll back,finalize the upgrade
hadoop dfsadmin -finalizeUpgrade
after this step, no roll back possible.

-----------------------------------
Checking contents of FSIMAGE 
hdfs oiv -i /data/namenode/current/fsiamge - o fsimage.txt
using this viewer,i can view the metadata structure
cat fsiamge.txt

--------------------------------------------------------
to have namenode data in specific directories and more locations in hdfs-site.xml

<property>
<name>dfs.name.dir</name>
<value>/data/namenode,/nfs/share(or any other directory location</value>
<final>true</final>
</property>

Here second location can be on different disk or nfs or on same disk
to setup NFS refer "Hadoop essential commands and parameters-2
----------------------------------------
to commision and decommision nodes add these to hdfs-site.xml
<property>
<name>dfs.hosts.include</name>
<value>/usr/local/hadoop/include</value>
<final>true</final>
</property>

<property>
<name>dfs.hosts.exclude</name>
<value>/usr/local/hadoop/exclude</value>
<final>true</final>
</property>

similarly add this paramter to yarn-site

<property>
<name>yarn.resourcemanager.nodes.include-path</name>
<value>/usr/local/hadoop/...include file path</value>
<final>true</final>
</property>

<property>
<name>yarn.resourcemanager.nodes.exclude-path</name>
<value>/usr/local/hadoop/...exclude file path</value>
<final>true</final>
</property>

and then add respective IPs in include and exclude.

and in had2
bin/hdfs dfsadmin -refreshNodes
bin/yarn rmadmin -refreshNodes

also update the slaves file accordingly

run balancer command to mv HDFS blocks to remaining datanodes.

Start daemons
-------------------------------------------------
to introduce trash
add in your core-site.xml in namenode
<property>
<name>fs.trash.interval</name>
<value>40</value>
</property>

-------------------
Understanding quota
$ hadoop fs -count -q /user/jsmith
none    inf    209715200    209715200    5   1   0
The output columns for fs -count -q are: QUOTA, REMAINING_QUOTA, SPACE_QUOTA, REMAINING_SPACE_QUOTA,
 DIR_COUNT, FILE_COUNT, CONTENT_SIZE, FILE_NAME.

 Hadoop checks space quotas during space allocation. This means that HDFS block size (here: 128MB) 
and 
the replication factor of the file
 (here: 3, i.e. the default value in the cluster set by the dfs.replication property) play an important role.
 In my case, this is what seems
 to have happened: When I tried to copy the local file to HDFS,

required_number_of_HDFS blocks * HDFS_block_size * replication_count
= 1 * 128MB * 3 = 384MB > 200MB.

hadoop fs -D dfs.replication=1 -copyFromLocal small-file.txt /user/jsmith

Now keep in mind that the Hadoop space quota always counts against the raw HDFS disk space consumed. So if you 
have a quota of 10MB, you can store only a single 1MB 
file if you set its replication to 10. Or you can store up to three 1MB files if their replication is set to 3. 
The reason why Hadoops quotas work like that is because the replication count of an HDFS file is a 
user-configurable setting. 
Though Hadoop ships with a default value of 3 it is up to the users to
 decide whether they want to keep this value or change it. 
And because Hadoop cant anticipate how 
users might be playing around with the replication setting for their files,
 it was decided that the Hadoop quotas always operate on the raw HDFS disk space consumed.
 Hadoop figured it would require a single block of 128MB size to store the small file. 
With replication factored in, the total space would be 3 * 128MB = 384 MB. 
And this would violate the space quota of 200MB.

