
HAsetup for apache core hadoop : 
----------------------------------
Using NFS
We need to make one machine as NFS so that active namenode can write edits to a shared location and
standby namenode can read from shared location.

for ubuntu:
On machine which you want make as your NFS.
sudo apt-get install nfs-kernel-server

to make sure there is no problem in mounting an NFS share,make sure nfs-common package is installed
on your clients(machines from where you will mount data on nfs) 
to install
nfs-common

sudo apt-get install nfs-common

sudo vi /etc/exports
path     *(rw,sync,no_root_sqash)
path needs to be created and ur admin user can be made owner of path.

sudo service nfs-kernel-server start
-----------------------
now going to each machine which will act as namenode
give the following command.

showmount -e ipaddress of ur nfs machine

this should show you path on nfs which was shared.

now create a mount point where you want edits to be written and which will be shared
say /mnt/filer and make hduser as owner of it.

now
mount -t nfs ipaddress of ur nfs machine:/shared path /mnt/filer

same needs to be done on other nn machines also.
=================
setting up on centos:
How To Set Up an NFS Mount on CentOS 6
About NFS (Network File System) Mounts

NFS mounts work to share a directory between several servers. 
This has the advantage of saving disk space, as the home directory is only kept on one server, 
and others can connect to it over the network. When setting up mounts,
 NFS is most effective for permanent fixtures that should always be accessible.

Setup
An NFS mount is set up between at least two servers. The machine hosting the shared network 
is called the server, while the ones that connect to it are called ‘clients’.

This tutorial requires 2 servers: one acting as the server and one as the client. 
We will set up the server machine first, followed by the client.
 The following IP addresses will refer to each one:

Master: 12.34.56.789

Client: 12.33.44.555

The system should be set up as root. You can access the root user by typing

sudo su
Setting Up the NFS Server
Step One—Download the Required Software

Start off by using apt-get to install the nfs programs.

yum install nfs-utils nfs-utils-lib

Subsequently, run several startup scripts for the NFS server:

chkconfig nfs on 
service rpcbind start
service nfs start

Step Two—Export the Shared Directory

The next step is to decide which directory we want to share with the client server. The chosen directory should then be added to the /etc/exports file, which specifies both the directory to be shared and the details of how it is shared.

Suppose we wanted to share the directory, /home.

We need to export the directory:

vi /etc/exports
Add the following lines to the bottom of the file, sharing the directory with the client:

/home           12.33.44.555(rw,sync,no_root_squash,no_subtree_check)
These settings accomplish several tasks:

rw: This option allows the client server to both read and write within the shared directory
sync: Sync confirms requests to the shared directory only once the changes have been committed.
no_subtree_check: This option prevents the subtree checking. When a shared directory 
is the subdirectory of a larger filesystem, nfs performs scans of every directory above it,
 in order to verify its permissions and details. Disabling the subtree check may increase the 
reliability of NFS, but reduce security.
no_root_squash: This phrase allows root to connect to the designated directory
Once you have entered in the settings for each directory, run the following command to export them:

exportfs -a

Setting Up the NFS Client
Step One—Download the Required Software

Start off by using apt-get to install the nfs programs.

yum install nfs-utils nfs-utils-lib

Step Two—Mount the Directories

Once the programs have been downloaded to the the client server, create the directory that will 
contain the NFS shared files

mkdir -p /mnt/nfs/home
Then go ahead and mount it

mount 12.34.56.789:/home /mnt/nfs/home
You can use the df -h command to check that the directory has been mounted. You will see it last on the list.

df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda               20G  783M   18G   5% /
12.34.56.789:/home       20G  785M   18G   5% /mnt/nfs/home
Additionally, use the mount command to see the entire list of mounted file systems.

mount
Your list should look something like this:

/dev/sda on / type ext4 (rw,errors=remount-ro)
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)
nfsd on /proc/fs/nfsd type nfsd (rw)
12.34.56.789:/home on /mnt/nfs/home type nfs (rw,noatime,nolock,bg,nfsvers=2,intr,tcp,actimeo=1800,addr=12.34.56.789)
Testing the NFS Mount
Once you have successfully mounted your NFS directory, you can test that it works by creating a file on the Client and checking its availability on the Server.

Create a file in the directory to try it out:

touch /mnt/nfs/home/example
You should then be able to find the files on the Server in the /home.

ls /home
You can ensure that the mount is always active by adding the directory to the fstab file on the client. 
This will ensure that the mount starts up after the server reboots.

vi /etc/fstab
12.34.56.789:/home  /mnt/nfs/home   nfs      auto,noatime,nolock,bg,nfsvers=3,intr,tcp,actimeo=1800 0 0
You can learn more about the fstab options by typing in:

man nfs
After any subsequent server reboots, you can use a single command to mount directories specified in the fstab file:

mount -a
You can check the mounted directories with the two earlier commands:

df -h
mount
Removing the NFS Mount
Should you decide to remove a directory, you can unmount it using the umount command:

cd
sudo umount /directory name
You can see that the mounts were removed by then looking at the filesystem again.

df -h
You should find your selected mounted directory gone.

=================

Setup these in two machines which will act as 
active and standby namenode.
When using no zookeeper
when using nfs, remember to setup nfs and mount /mnt/filer to nfs path
In my case aj1-active
aj4-standby

in core-site.xml
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://mycluster</value>
<description>The name of default file system</description>
</property>
</configuration>

in hdfs-site.xml

<configuration>
<property>
<name>dfs.nameservices</name>
<value>mycluster</value>
</property>

<property>
<name>dfs.replication</name>
<value>2</value>
<description>to specifiy replication</description>
</property>

<property>
<name>dfs.namenode.name.dir</name>
<value>file:/wkdaydec1ha/name</value>
<final>true</final>
</property>

<property>
<name>dfs.datanode.data.dir</name>
<value>file:/wkdaydec1ha/data1</value>
<final>true</final>
</property>

<property>
<name>dfs.ha.namenodes.mycluster</name>
<value>nn1,nn2</value>
</property>

<property>
<name>dfs.namenode.rpc-address.mycluster.nn1</name>
<value>aj1:9000</value>
</property>

<property>
<name>dfs.namenode.rpc-address.mycluster.nn2</name>
<value>aj4:9000</value>
</property>

<property>
<name>dfs.namenode.http-address.mycluster.nn1</name>
<value>aj1:50070</value>
</property>

<property>
<name>dfs.namenode.http-address.mycluster.nn2</name>
<value>aj4:50070</value>
</property>

<property>
<name>dfs.namenode.shared.edits.dir</name>
<value>file:///mnt/filer</value>
</property>

<property>
<name>dfs.client.failover.proxy.provider.mycluster</name>
<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>

<property>
<name>dfs.ha.fencing.methods</name>
<value>sshfence</value>
</property>

<property>
<name>dfs.ha.fencing.ssh.private-key-files</name>
<value>/home/hduser/.ssh/id_rsa</value>
</property>

<property>
<name>dfs.ha.fencing.methods</name>
<value>sshfence
       shell(/bin/true)
</value>
</property>

</configuration>
======================
After updating
steps if setting up a new HA cluster
format 1st nn
start 1st nn
do a bootstrap command on 2nd nn ( command is below)
start 2nd nn
check status of both nn's using getservice command (command below)
both would be in standby status ( as no zk used)
make one of them active using transition command( command below)
start ur other daemons
check if you see edits_inprogress in metadata path of active nn

Note** if enabling HA in existing hadoop 2 cluster which had only one nn
stop cluster
don't do a formatting
do steps as mentioned above from --start 1st nn

If using zookeeper & zkfc component
The configuration of automatic failover requires the addition of two new parameters 
to your configuration. In your hdfs-site.xml file, add:

 <property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>

This specifies that the cluster should be set up for automatic failover. 
In your core-site.xml file, add:

 <property>
   <name>ha.zookeeper.quorum</name>
   <value>zk1.example.com:2181,zk2.example.com:2181,zk3.example.com:2181</value>
 </property>

In new cluster:
format 1st nn
start 1st nn
do a bootstrap command on 2nd nn ( command is below)
start 2nd nn
stop both of them
Issue this command to initialize ha status in zk ( command can be given on any nn)
hdfs zkfc -formatZK
Give a Start-dfs.sh commands ( it should start nns, zkfc and dns)
Note** when using zookeeper, zookeeper to be installed on every machine from
zookeeper.apache.org

do the steps to setup zookeeper and start zookeeper

{Steps to setup zookeeper on every node:
Download zookeeper tar file
Untar it in say '/usr/local/'
create a link
$cd /usr/local
$sudo ln -s zookeeperdir zookeeper
$sudo chown -R hduser:hadoop zookeeper*
update .bashrc with path to zookeeper

$cd /usr/local/zookeeper
$cp conf/zoo_sample.cfg conf/zoo.cfg
edit zoo.cfg to have zookeeper data go in /var/lib/zookeeper
dont forget to create zookeeper dir and change its ownership
$bin/zkServer.sh start ---- to start zookeeper
=====================================
HA commands ( in Apache Hadoop cluster)

hdfs namenode -initializeSharedEdits -force
hdfs hdfs namenode -bootstrapStandby -force
hdfs haadmin -failover --forceactive nn1 nn2
hdfs haadmin -transitionToActive nn1
hdfs haadmin -getServiceState nn1/nn2
===================================
